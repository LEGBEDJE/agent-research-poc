import streamlit as st
import asyncio
import os
import nest_asyncio
from pydantic_ai import Agent
from pydantic_ai.models.groq import GroqModel
from pydantic import BaseModel, Field

# Indispensable pour l'ex√©cution asynchrone dans Streamlit
nest_asyncio.apply()

# --- CONFIGURATION UI ---
st.set_page_config(page_title="AI Research Agent", page_icon="üî¨", layout="centered")

st.title("üî¨ Agent de Recherche Autonome")
st.markdown("""
Cette application d√©montre les capacit√©s d'un **Agent IA** √† utiliser des outils externes 
pour r√©pondre √† des questions techniques complexes.
""")

# --- LOGIQUE DE L'AGENT ---

# D√©finition de la structure de sortie pour garantir la fiabilit√© des donn√©es
class AgentOutput(BaseModel):
    answer: str = Field(description="La r√©ponse finale structur√©e")
    used_tools: bool = Field(description="Indique si l'outil de recherche a √©t√© consult√©")

# Sidebar pour la s√©curit√© (Cl√© API)
with st.sidebar:
    st.header("üîë Authentification")
    user_api_key = st.text_input("Cl√© API Groq", type="password", help="Obtenez une cl√© gratuite sur console.groq.com")
    st.info("Le mod√®le utilis√© est **Llama-3.3-70b-Versatile**.")

if not user_api_key:
    st.warning("Veuillez entrer votre cl√© API Groq dans la barre lat√©rale.")
    st.stop()

# Initialisation du mod√®le et de l'agent
try:
    os.environ['GROQ_API_KEY'] = user_api_key
    model = GroqModel('llama-3.3-70b-versatile')
    
    system_prompt = """Tu es un expert en R&D IA. 
    Pour toute question technique (RAG, Pydantic-AI, vLLM, Agents), utilise SYSTEMATIQUEMENT 
    l'outil 'search_technical_doc' pour garantir l'exactitude des informations."""
    
    agent = Agent(model=model, system_prompt=system_prompt)

    # D√©finition de l'outil de recherche (Simulated RAG)
    @agent.tool
    async def search_technical_doc(ctx, topic: str) -> str:
        """Recherche des d√©finitions techniques dans la documentation interne."""
        knowledge_base = {
            "rag": "RAG (Retrieval-Augmented Generation) : architecture combinant recherche vectorielle et LLM pour r√©duire les hallucinations.",
            "pydantic-ai": "Framework Python de Pydantic pour b√¢tir des agents type-safe et robustes pour la production.",
            "vllm": "Moteur de serving haute performance optimis√© pour le d√©ploiement de LLM (KV cache, batching).",
            "agent": "Syst√®me autonome capable de raisonner, d'utiliser des outils et d'agir pour atteindre un objectif."
        }
        return knowledge_base.get(topic.lower(), f"Le sujet '{topic}' n'est pas document√© en interne.")

except Exception as e:
    st.error(f"Erreur d'initialisation : {e}")
    st.stop()

# --- INTERFACE DE CHAT ---
if "messages" not in st.session_state:
    st.session_state.messages = []

# Affichage de l'historique
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Entr√©e utilisateur
if prompt := st.chat_input("Ex: Explique-moi les avantages du vLLM"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("L'agent consulte la documentation..."):
            try:
                # Ex√©cution asynchrone de l'agent
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                
                enhanced_prompt = f"Utilise tes outils pour r√©pondre √† : {prompt}"
                result = loop.run_until_complete(agent.run(enhanced_prompt))
                
                response_text = str(result.output)
                st.markdown(response_text)
                st.session_state.messages.append({"role": "assistant", "content": response_text})
            except Exception as e:
                st.error(f"Erreur d'ex√©cution : {e}")
